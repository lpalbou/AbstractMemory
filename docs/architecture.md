# AbstractMemory: Technical Architecture

**Why AbstractMemory is designed the way it is**

## ğŸ§  Core Design Philosophy

### The Context Window Problem
Traditional AI is limited by context windows (~100k tokens). AbstractMemory solves this by implementing human-like memory:

- **Conscious Memory** = Active context (recent interactions)
- **Subconscious Memory** = Persistent storage (unlimited history)
- **Selective Recall** = Retrieve relevant memories when needed

### Memory as Human Consciousness
```
Human Memory              AbstractMemory Implementation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Working Memory            â†’ Core + Working Memory (always active)
Long-term Memory          â†’ Semantic + Episodic Memory (selective)
Automatic Recall          â†’ Context injection based on relevance
```

## ğŸ—ï¸ Four-Tier Memory Architecture

### Tier Hierarchy & Purpose

| Tier | Purpose | Always Available | Persistence | Example |
|------|---------|------------------|-------------|---------|
| **Core** | Agent identity/persona | âœ… Yes (system prompt) | Permanent | "I specialize in Python development" |
| **Working** | Recent context | âœ… Yes | Session | Last 10-15 interactions |
| **Semantic** | Validated facts | âœ… Yes | Persistent | "Alice prefers FastAPI" (confidence â‰¥ 0.8) |
| **Episodic** | Historical events | âŒ Query-dependent | Persistent | "Yesterday Alice asked about databases" |

### Memory Flow & Consolidation

```
User Input â†’ Working Memory â†’ Semantic Memory (if validated)
     â†“              â†“              â†“
Storage â”€â”€â”€â”€â†’ Episodic Memory â†â”€â”€â”€â”€â”€â”€â”˜
     â†“              â†“
Vector Search â†â”€â”€â”€â”€â”€â”´â”€â”€â”€ Context Retrieval
```

**Validation Process:**
1. Facts start in Working Memory
2. Repeated mentions increase confidence
3. High confidence (â‰¥0.8) â†’ moves to Semantic Memory
4. All interactions archived in Episodic Memory

### Memory Injection Strategy (MemGPT Pattern)

**System Prompt Structure:**
```
[Base System Prompt]

=== AGENT IDENTITY ===
[Core Memory - Always Present]

=== MEMORY CONTEXT ===
[Selective Context Based on Query Relevance]
â”œâ”€ User Profile (relationship, key facts)
â”œâ”€ Recent Context (working memory)
â”œâ”€ Learned Facts (semantic memory)
â””â”€ Relevant Events (episodic memory)
```

**Why Core Memory is in System Prompt:**
- Agent identity must be persistent across all interactions
- Core memory shapes how the agent interprets everything
- System prompt is never truncated unlike regular context

## ğŸ” Embedding Strategy & Consistency

### The Golden Rule: One Model Forever

**Critical Requirement:** Same embedding model for entire storage lifespan

```python
# âœ… CORRECT: Consistent embeddings
embedder = create_sentence_transformer_provider("all-MiniLM-L6-v2")
session = MemorySession(provider, embedding_provider=embedder)

# âŒ WRONG: Changing models breaks search
# Day 1: all-MiniLM-L6-v2
# Day 2: bge-base-en-v1.5  â† Vector space completely different!
```

### Recommended Model: all-MiniLM-L6-v2

**Why this model:**
- **Accuracy**: 100% P@5, R@5, F1 scores on AbstractMemory test suite
- **Performance**: 13ms embedding generation (vs 40ms for larger models)
- **Efficiency**: 384D vectors (vs 768D) = 50% storage savings
- **Balance**: Best accuracy/performance ratio

### LLM vs Embedding Provider Separation

```python
# Different purposes, different providers
llm_provider = create_llm("ollama", model="qwen3-coder:30b")      # Text generation
embedding_provider = create_sentence_transformer_provider(...)    # Semantic search

# LLM provider can change freely
# Embedding provider must stay consistent per storage
```

## ğŸ’¾ Storage Architecture

### Storage Options Comparison

| Backend | Observable | Searchable | Performance | Use Case |
|---------|------------|------------|-------------|----------|
| **None** | âŒ | âŒ | Fastest (0ms) | Development/Testing |
| **Markdown** | âœ… | âŒ | Fast (~5ms) | Debugging/Transparency |
| **LanceDB** | âŒ | âœ… | Fast (~100ms) | Production Search |
| **Dual** | âœ… | âœ… | Good (~105ms) | **Recommended** |

### Dual Storage Benefits

**Markdown Storage (Observable):**
- Human-readable memory evolution
- Version controllable with git
- Perfect for debugging and transparency
- Organized by user/date/interaction type

**LanceDB Storage (Searchable):**
- Vector similarity search
- SQL-like querying capabilities
- Sub-second search across thousands of memories
- Automatic indexing and optimization

### Storage Structure
```
memory/
â”œâ”€â”€ markdown/
â”‚   â”œâ”€â”€ verbatim/alice/2025/09/25/10-30-45_python_abc123.md
â”‚   â”œâ”€â”€ experiential/2025/09/25/10-31-02_learning_def456.md
â”‚   â””â”€â”€ links/2025/09/25/int_abc123_to_note_def456.json
â””â”€â”€ vectors.db  (LanceDB vector storage)
```

## âš¡ Performance Characteristics

### Real-World Benchmarks
- **Memory Context Injection**: ~5ms (system prompt enhancement)
- **Fact Extraction**: ~2ms (pattern matching from user input)
- **Embedding Generation**: ~13ms (all-MiniLM-L6-v2 model)
- **Vector Search**: ~100ms (1000+ stored interactions)
- **Storage Write**: ~15ms (dual storage)

### Scalability Limits
- **Working Memory**: 10-15 items (context window management)
- **Semantic Facts**: No limit (confidence-based retrieval)
- **Episodic Events**: No limit (query-dependent retrieval)
- **Vector Storage**: 100K+ interactions tested

## ğŸ¤– Autonomous Agent Integration

### Memory Tools Architecture

When `enable_memory_tools=True`, agents get:

```python
@tool("Search your memory for specific information")
def search_memory(query, user_id, memory_types, limit=5):
    # Cross-tier search with relevance ranking

@tool("Remember an important fact")
def remember_fact(fact, user_id):
    # Direct injection into semantic memory

@tool("Update your core identity")
def update_core_memory(block, content, reason):
    # Permanent agent identity modification
```

### Self-Evolution Mechanism
1. **Experience Collection**: All interactions stored
2. **Pattern Recognition**: Failure/success tracking
3. **Identity Update**: Agent can modify core memory
4. **Knowledge Accumulation**: Facts consolidate over time

### Tool Registration Flow
```python
config = MemoryConfig.agent_mode()
session = MemorySession(provider, default_memory_config=config)
# â†’ Automatically registers 6 memory tools
# â†’ Tools filtered by allowed_memory_operations
# â†’ Self-editing controlled by enable_self_editing
```

## ğŸ›ï¸ Architectural Principles

### 1. Progressive Complexity
- **Simple by default**: MemorySession() works immediately
- **Powerful when configured**: Full autonomous agent capabilities
- **No over-engineering**: Memory complexity matches agent needs

### 2. Separation of Concerns
- **Memory Management**: AbstractMemory responsibility
- **Text Generation**: LLM provider responsibility
- **Tool Execution**: AbstractCore responsibility
- **Storage**: Pluggable backends (Markdown/LanceDB)

### 3. Human-Centric Design
- **Observable**: Markdown storage shows memory evolution
- **Debuggable**: Clear separation of memory tiers
- **Intuitive**: Memory works like human consciousness

### 4. Production Ready
- **Real Testing**: 200+ tests with real LLMs (no mocks)
- **Performance Optimized**: Sub-100ms operations
- **Error Handling**: Graceful degradation on failures
- **Monitoring**: Debug logging and metrics

## ğŸ”¬ Research Foundations

### State-of-the-Art Patterns
- **MemGPT/Letta**: Core memory in system prompt
- **Temporal Knowledge Graphs**: Who/when context
- **MongoDB Vector Search**: Hybrid retrieval
- **LangGraph Memory**: Metadata-driven filtering

### Novel Contributions
- **Automatic Fact Extraction**: Pattern-based learning from interactions
- **Dual Storage Architecture**: Observable + searchable memory
- **Progressive Memory Tiers**: Working â†’ Semantic â†’ Episodic flow
- **Context Window Transcendence**: Unlimited memory with selective recall

---

**AbstractMemory: Solving the context window problem with human-like memory architecture** ğŸ§ âœ¨

